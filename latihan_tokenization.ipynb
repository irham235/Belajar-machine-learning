{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNe2TMv2H6GtU2Kt4hwxP9s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/irham235/Belajar-machine-learning/blob/main/latihan_tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "ym8aFQiXIV3M"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kemudian buat objek tokenizer dengan memanggil fungsi tokenizer dan melengkapi parameternya. Parameter num_words adalah jumlah kata yang akan dikonversi ke dalam token/bilangan numerik. Jika parameter num_words diatur sebanyak 15, hanya 15 kata yang paling sering muncul. 15 kata tersebut akan ditokenisasi dari seluruh kata pada dataset."
      ],
      "metadata": {
        "id": "hYeYyWYbIw_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sedangkan parameter oov_token adalah parameter yang berfungsi untuk mengganti kata-kata yang tidak ditokenisasi menjadi karakter tertentu. Pada praktiknya, lebih baik untuk mengganti kata yang tidak dikenali dengan suatu kata tertentu dibanding melewatkan kata tersebut untuk mengurangi informasi yang hilang. Hal inilah yang dapat dilakukan dengan menambahkan parameter OOV."
      ],
      "metadata": {
        "id": "du8YbEQEI5bD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words= 15, oov_token='-')"
      ],
      "metadata": {
        "id": "4kNYKkR_IwZi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lalu, buat teks yang akan ditokenisasi dan dipakai untuk pelatihan model."
      ],
      "metadata": {
        "id": "6W_0OUqVJFMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "teks = ['Saya suka programming',\n",
        "        'Programming sangat menyenangkan!',\n",
        "        'Machibe Learning berbeda dengan pemrograman konvensional']"
      ],
      "metadata": {
        "id": "8Ekp7xZBJKqt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Untuk melakukan tokenisasi, panggil fungsi fit_on_text() pada objek tokenizer dan isi teks kita sebagai argumennya.\n",
        "\n"
      ],
      "metadata": {
        "id": "mxOQLmtPJY-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.fit_on_texts(teks)"
      ],
      "metadata": {
        "id": "bg7gwY8pJYcr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kemudian, kita akan mengubah text yang telah dibuat sebelumnya ke dalam bentuk sequence menggunakan fungsi text_to_sequences."
      ],
      "metadata": {
        "id": "_TptYGOPJg_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = tokenizer.texts_to_sequences(teks)"
      ],
      "metadata": {
        "id": "_QqyrfLIJfl7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Untuk melihat hasil tokenisasi, kita bisa memanggil atribut word_index dari objek tokenizer. Atribut word index mengembalikan dictionary berupa kata sebagai key dan token atau nilai numeriknya sebagai value. Perlu diperhatikan bahwa tanda baca dan huruf kapital tidak diproses oleh tokenizer. Contohnya kata “Selamat!” dan “SELAMAT” akan diperlakukan sebagai kata yang sama. Hasil dari cell di bawah menunjukkan bahwa kata-kata yang out-of-vocabulary akan diberi token bernilai 1."
      ],
      "metadata": {
        "id": "N3Iu_m5uJySd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9hXDacVJzLR",
        "outputId": "4afb9073-e9af-4696-cf0c-621d923debd8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'-': 1, 'programming': 2, 'saya': 3, 'suka': 4, 'sangat': 5, 'menyenangkan': 6, 'machibe': 7, 'learning': 8, 'berbeda': 9, 'dengan': 10, 'pemrograman': 11, 'konvensional': 12}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output kode di bawah merupakan contoh penggunaan token untuk mengubah kalimat ke dalam bentuk numerik. Pada contoh tersebut, kata ‘belajar’, ‘sejak’, dan ‘SMP’ ditandai dengan nilai \"1\". Hal ini menunjukkan bahwa kata-kata tersebut tidak terdapat pada dictionary yang sebelumnya telah dibuat (OOV). Tanpa OOV, kata yang tidak memiliki token tidak dimasukkan pada sequence. Jika kita menggunakan OOV, maka setiap kata yang tidak memiliki token akan diberikan token yang seragam. Dengan OOV, informasi urutan setiap kata pada kalimat tidak hilang."
      ],
      "metadata": {
        "id": "iwgJwToQKPWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.texts_to_sequences(['Saya suka programming']))\n",
        "print(tokenizer.texts_to_sequences(['Saya suka belajar programing sejak SMP']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jia4MLjZKQX9",
        "outputId": "b2e29344-a2b4-4910-b227-9fd0bc8c6434"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3, 4, 2]]\n",
            "[[3, 4, 1, 1, 1, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setelah tokenisasi, untuk mengubah kalimat ke dalam nilai-nilai yang sesuai dapat dengan menggunakan fungsi text_to_sequence() dan masukkan parameter dari teks kita. Ketika sequence telah dibuat, hal yang perlu kita lakukan adalah padding. Yup, padding adalah proses untuk membuat setiap kalimat pada teks memiliki panjang yang seragam. Sama seperti melakukan resize gambar, agar resolusi setiap gambar sama besar."
      ],
      "metadata": {
        "id": "rNN9K1UmKv6I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Untuk menggunakan padding, kita perlu memanggil library pad_sequence terlebih dahulu. Kemudian, panggil fungsi pad_sequence() dan masukkan sequence hasil tokenisasi sebagai parameternya."
      ],
      "metadata": {
        "id": "QOf1yXmOK2yv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "sequences_samapanjang = pad_sequences(sequences)"
      ],
      "metadata": {
        "id": "k9mcmsGYKwxL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "setelah melakukan padding, setiap sequence akan memiliki panjang yang sama. Padding dapat melakukan ini dengan menambahkan 0 secara default pada awal sequence yang lebih pendek"
      ],
      "metadata": {
        "id": "4Ear1ue-LKtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(sequences_samapanjang)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Q2LzniILM4F",
        "outputId": "4969788d-801d-4d40-acb7-a35ab64ea7a9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  0  0  3  4  2]\n",
            " [ 0  0  0  2  5  6]\n",
            " [ 7  8  9 10 11 12]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jika kita ingin merubah sehingga 0 ditambahkan di akhir sequence, kita dapat menggunakan parameter padding dengan nilai ‘post’. Selain itu kita dapat mengatur parameter maxlen (panjang maksimum setiap sequence) dengan nilai yang kita inginkan. Jika kita mengisi nilai 5, maka panjang sebuah sequence tidak akan melebihi 5."
      ],
      "metadata": {
        "id": "v9kXrBKuLc60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences_samapanjang = pad_sequences(sequences,\n",
        "                                      padding='post',\n",
        "                                      maxlen=5)"
      ],
      "metadata": {
        "id": "wil4W90iLd9g"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jika teks kita memiliki panjang lebih dari nilai parameter maxlen misalnya 5, maka secara default nilai dari sequence akan diambil 5 nilai terakhir atau 5 kata terakhir saja dari setiap kalimat (mengabaikan kata sebelumnya). Untuk mengubah pengaturan ini dan mengambil 5 kata awal dari tiap kalimat, kita dapat menggunakan parameter truncating dan mengisi nilai ‘post’."
      ],
      "metadata": {
        "id": "-CntU8OWLwBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences_samapanjang = pad_sequences(sequences,\n",
        "                                      padding='post',\n",
        "                                      maxlen=5,\n",
        "                                      truncating='post')"
      ],
      "metadata": {
        "id": "hwMaPufzLvfT"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}